{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f065ed9-ccc3-4431-9b53-c5637fd606b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from utils import DeepSortMock, drawDetection, random_bbox\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48044ede-ca06-4523-b377-7cfbcd271304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/nilez/.local/share/virtualenvs/ML.me-i4hkX-4D/lib/python3.8/site-packages (1.9.0)\n",
      "Requirement already satisfied: torchvision in /Users/nilez/.local/share/virtualenvs/ML.me-i4hkX-4D/lib/python3.8/site-packages (0.10.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/nilez/.local/share/virtualenvs/ML.me-i4hkX-4D/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /Users/nilez/.local/share/virtualenvs/ML.me-i4hkX-4D/lib/python3.8/site-packages (from torchvision) (8.3.1)\n",
      "Requirement already satisfied: numpy in /Users/nilez/.local/share/virtualenvs/ML.me-i4hkX-4D/lib/python3.8/site-packages (from torchvision) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd095b79-bc40-4a40-a320-a05a24f80c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = [\"grandma_A\", \"grandma_B\"]\n",
    "feeds = [cv2.VideoCapture(f\"raw_data/videos/{file}.mp4\") for file in data_files]\n",
    "detectors = [DeepSortMock(f\"raw_data/tracks/{file}.cleaned.pb\") for file in data_files]\n",
    "feed_identities = [dict() for _ in data_files] \n",
    "\n",
    "output_frames = []\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "frames_left = True\n",
    "frame_num = 0 \n",
    "\n",
    "try:\n",
    "    while frames_left:\n",
    "        frame_num += 1\n",
    "        output_frames = [ ]\n",
    "    \n",
    "        for i, (feed, detector, identities) in enumerate(zip(feeds, detectors, feed_identities)):\n",
    "            frames_left, frame = feed.read()\n",
    " \n",
    "            if not frames_left: break\n",
    "            \n",
    "            if frame_num % 24 == 0:\n",
    "                for ID in identities:\n",
    "                    start, stop = identities[ID][0], identities[ID][-1]\n",
    "                    identities[ID] = [stop]\n",
    "                    \n",
    "                    # Get Pixels per Frame Velocity\n",
    "                    num_a, c_a = start\n",
    "                    num_b, c_b = stop \n",
    "                    if num_a == num_b: continue\n",
    "\n",
    "                    c_d = ((c_a[0] - c_b[0] / (num_a - num_b)), (c_a[1] - c_b[1] / (num_a - num_b)))\n",
    "                    \n",
    "                    a = np.zeros(len(data_files))\n",
    "                    a[i] = 1\n",
    "                    \n",
    "                    feat_x = (num_b,) + tuple(a) + c_b + c_d\n",
    "                    \n",
    "                    x_data.append(feat_x)\n",
    "                    y_data.append(ID)\n",
    "                    \n",
    "            updated_ids = set()        \n",
    "            people = detector.update(frame)\n",
    "            # Grab Positive Person images to train seperators  \n",
    "            for person in people:\n",
    "                p1, p2, cat, ID = person\n",
    "                updated_ids.add(ID)\n",
    "                \n",
    "                # Center coordinates\n",
    "                c = ((p1[0] + (abs(p2[0] - p1[0]) / 2)), (p1[1] + (abs(p2[1] - p1[1]) / 2)))\n",
    "                \n",
    "                # Normalized Centers\n",
    "                c_n = (c[0] / frame.shape[0], c[1] / frame.shape[1])\n",
    "                \n",
    "                if ID not in identities: identities[ID] = []\n",
    "                identities[ID].append((frame_num, c_n))\n",
    "\n",
    "                # Draw a circle with blue line borders of thickness of 2 px\n",
    "                frame = cv2.circle(frame, (int(c[0]), int(c[1])), 20, (255, 0, 0), 5)\n",
    "                frame = drawDetection(frame, (p1, p2, cat, ID), info = frame_num)\n",
    "                \n",
    "            output_frames.append(frame) \n",
    "        try:\n",
    "            cv2.imshow(\"Joined Product\",\n",
    "                np.concatenate(tuple(output_frame for output_frame in output_frames), axis = 1))\n",
    "        except :\n",
    "            pass\n",
    "        finally:\n",
    "            cv2.waitKey(1)\n",
    "                \n",
    "finally:\n",
    "    [cap.release() for cap in feeds]\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d870f8a5-a545-47a9-b258-4829043870a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A categorical input for every video feed\n",
    "# the X and Y vel across the latest N detections\n",
    "# The X, Y center of the most recent detection\n",
    "\n",
    "def generate_training_data(x, y, output_size = 10000):\n",
    "    x_out = []\n",
    "    y_out = [] \n",
    "  \n",
    "    while len(y_out) < output_size:\n",
    "        x_1, x_2 = np.random.randint(len(x), size=(2))\n",
    "        if abs(x[x_1][0] - x[x_2][0]) > (24 * 4): continue\n",
    "        \n",
    "        x_out.append(x[x_1][1:] + (abs(x[x_1][0] - x[x_2][0]),) + x[x_2][1:])\n",
    "        y_out.append([y[x_1] == y[x_2]])\n",
    "    return np.array(x_out), np.array(y_out, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30800bf6-5b2b-4116-99fc-cf363e0167c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(data_files) + 2 + 2\n",
    "\n",
    "net = NeuralNetwork()\n",
    "net.add(Dense(input_size))\n",
    "net.add(Dense(32))\n",
    "net.add(Dense(16))\n",
    "\n",
    "err = NeuralNetwork()\n",
    "err.add(Dense(net._layers[-1].outputSize))\n",
    "err.add(Dense(16))\n",
    "err.add(Dense(10))\n",
    "err.add(Dense(1))\n",
    "\n",
    "s_net = SiameseNetwork(loss_type=\"model\")\n",
    "\n",
    "s_net.set_main_model(net)\n",
    "s_net.set_error_model(err)\n",
    "\n",
    "net.set_training_set(X,Y)\n",
    "\n",
    "# net.train(epochs = 1, batch_size = 200)\n",
    "# net.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ffdd5-962a-485f-9511-aede2d453d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_me.architectures import NeuralNetwork\n",
    "from ml_me.layers import Dense, Layer\n",
    "\n",
    "output_size = 10\n",
    "split = output_size // 2\n",
    "X, Y = generate_training_data(x_data, y_data, output_size = output_size)\n",
    "\n",
    "net = NeuralNetwork()\n",
    "net.add(Dense(13))\n",
    "net.add(Dense(32))\n",
    "net.add(Dense(16))\n",
    "net.add(Dense(1)) # Not Implemented, activation=\"softmax\")\n",
    "\n",
    "net.set_training_set(X[split:],Y[split:])\n",
    "net.train(epochs = 1000, batch_size = 200)\n",
    "print(f\"Recall: {net.get_recall()} Accuracy: {round(net.get_acc(X[:split], Y[:split]), 3)}\")\n",
    "plt.plot(range(len(net.losses)),net.losses)\n",
    "plt.title(\"Loss vs. Epochs\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf0e937-8c9d-42f8-b68d-a1632d9c9efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = [\"grandma_A\", \"grandma_B\"]\n",
    "feeds = [cv2.VideoCapture(f\"raw_data/videos/{file}.mp4\") for file in data_files]\n",
    "detectors = [DeepSortMock(f\"raw_data/tracks/{file}.cleaned.pb\") for file in data_files]\n",
    "feed_identities = [dict() for _ in feeds] \n",
    "\n",
    "# DEBUG \n",
    "output_frames = []\n",
    "\n",
    "samples = []\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "frames_left = True\n",
    "frame_num = 0 \n",
    "\n",
    "try:\n",
    "    while frames_left:\n",
    "        frame_num += 1\n",
    "        output_frames = [ ]\n",
    "    \n",
    "        for i, (feed, detector, identities) in enumerate(zip(feeds, detectors, feed_identities)):\n",
    "            updated_ids = set()\n",
    "            frames_left, frame = feed.read()\n",
    " \n",
    "            if not frames_left: break\n",
    "            \n",
    "            if frame_num % 24 == 0:\n",
    "                for ID in identities:\n",
    "                    start, stop = identities[ID][0], identities[ID][-1]\n",
    "                    identities[ID] = [stop]\n",
    "                    # Get Pixels per Frame Velocity\n",
    "                    num_a, c_a = start\n",
    "                    num_b, c_b = stop \n",
    "                    if num_a == num_b: continue\n",
    "\n",
    "                    c_d = ((c_a[0] - c_b[0]/(num_a - num_b)), (c_a[1] - c_b[1]/(num_a - num_b)))\n",
    "                    \n",
    "                    a = np.zeros(len(data_files))\n",
    "                    a[i] = 1\n",
    "                    \n",
    "                    feat_x = (num_b,) + tuple(a) + c_b + c_d\n",
    "                    \n",
    "                    x_data.append(feat_x)\n",
    "                    y_data.append(ID)\n",
    "                    \n",
    "            people = detector.update(frame)\n",
    "            # Grab Positive Person images to train seperators  \n",
    "            for person in people:\n",
    "                p1, p2, cat, ID = person\n",
    "                updated_ids.add(ID)\n",
    "                \n",
    "                # Normalized Centers\n",
    "                c = ((p1[0] + (abs(p2[0] - p1[0]) / 2)), (p1[1] + (abs(p2[1] - p1[1]) / 2)))\n",
    "                c_n = (c[0] / frame.shape[0], c[1] / frame.shape[1])\n",
    "                # Center coordinates\n",
    "                center_coordinates = (int(c[0]), int(c[1]))\n",
    "                if ID not in identities: identities[ID] = []\n",
    "                identities[ID].append((frame_num, c_n))\n",
    "\n",
    "                # Draw a circle with blue line borders of thickness of 2 px\n",
    "                frame = cv2.circle(frame, center_coordinates, 20, (255, 0, 0), 5)\n",
    "                frame = drawDetection(frame, (p1, p2, cat, ID), info = frame_num)\n",
    "        \n",
    "            output_frames.append(frame) \n",
    "            \n",
    "        try:\n",
    "            cv2.imshow(\"Joined Product\",\n",
    "                np.concatenate(tuple(output_frame for output_frame in output_frames), axis = 1))\n",
    "        except :\n",
    "            pass\n",
    "        cv2.waitKey(1)\n",
    "                \n",
    "finally:\n",
    "    [cap.release() for cap in feeds]\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed7504db-9e6e-4a4f-b5df-eabaa83fa010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.detections = x \n",
    "        self.labels = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.detections[idx], self.labels[idx]\n",
    "X, Y = generate_training_data(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f2b83-7b01-4135-a9a4-0302ec88e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Multilayer Perceptron.\n",
    "'''\n",
    "\n",
    "class MLP(nn.Module):\n",
    " \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          # nn.Flatten(),\n",
    "          nn.Linear(32 * 32 * 3, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Prepare CIFAR-10 dataset\n",
    "dataset = CustomDataset(X,Y)# CIFAR10(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
    "\n",
    "# Initialize the MLP\n",
    "mlp = MLP()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "\n",
    "# Run the training loop\n",
    "for epoch in range(0, 5): # 5 epochs at maximum\n",
    "\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # Get inputs\n",
    "        inputs, targets = data\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                  (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb29b42f-89fb-44db-86a0-3909a7896d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_me.architectures import NeuralNetwork\n",
    "from ml_me.layers import Dense, Layer\n",
    "\n",
    "class SiameseNetwork(NeuralNetwork):\n",
    "    def __init__(self, loss_type = \"dist\"):\n",
    "        self._model = None\n",
    "        self._error_model = None\n",
    "        self.margin = 3\n",
    "        self.loss_type = loss_type \n",
    "        \n",
    "    def set_main_model(self, model):\n",
    "        self._model = model\n",
    "        # TODO: Validate Model\n",
    "        pass\n",
    "    def set_error_model(self, model):\n",
    "        assert(self.loss_type == \"model\")\n",
    "         self._error_model = model\n",
    "    def add():\n",
    "        pass\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def backward(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, epochs = 1000, batch_size = 32, resolution = 10):\n",
    "        batch_idx = np.random.choice(len(self.training_data), batch_size)\n",
    "        data = np.array([self.training_data[y] for y in batch_idx])\n",
    "        labels = np.array([self.training_labels[t] for t in batch_idx])\n",
    "        if self.loss_type == \"model\":\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "        # Get inputs\n",
    "        self._model.forward()\n",
    "        self._model.backward()\n",
    "        \n",
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, activation = \"relu\", margin = 1, **kwargs):\n",
    "        Layer.__init__(self, \"Triplet\", **kwargs)\n",
    "        self.margin = 1\n",
    "        self.outputSize = 1\n",
    "        self.__frame_count = 0 \n",
    "        self.__embedding_memory = []\n",
    "        self.__label_memory = []\n",
    "    \n",
    "    def init(self, inputSize):\n",
    "        pass\n",
    "    \n",
    "    def calc_euclidian(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, labels):\n",
    "        # Generate Triplets \n",
    "        self.error = 1\n",
    "        self.delta = self.error * self.activation_prime(self.outputs)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
