{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "import random\n",
    "\n",
    "def tile_img(image, rows = 5, cols = 5):\n",
    "    image = Image.fromarray(image)\n",
    "    imgwidth, imgheight = image.size\n",
    "    height = imgheight // rows\n",
    "    width = imgwidth // cols\n",
    "    tiles = [ ]\n",
    "    for i in range(0, cols):\n",
    "        for j in range(0, rows):\n",
    "            box = (j * width, i * height, (j + 1) * width, (i + 1) * height)\n",
    "            a = image.crop(box)\n",
    "            tiles.append(np.array(a))\n",
    "    return tiles \n",
    "\n",
    "def get_tiled_adj_lists(rows = 5, cols = 5):\n",
    "    neighbor_sets = []\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            neighbors = []\n",
    "            for (x,y) in [(-1,-1),(-1,0),(-1,1),(0,-1),(0,1),(1,-1),(1,0),(1,1)]: # (0,0) exlcuded\n",
    "                if (0 <= (row + x) < rows) and (0 <= (col + y) < cols):\n",
    "                    idx  = ((row + x) * cols) + col + y\n",
    "                    neighbors.append(idx)\n",
    "            neighbor_sets.append(neighbors)\n",
    "    return neighbor_sets\n",
    "\n",
    "def localize_corr_matrix(corr, num_images, rows, cols, include_self = False):\n",
    "    adj_lists = get_tiled_adj_lists(rows, cols)\n",
    "    nodes_per_img = rows * cols \n",
    "\n",
    "    localized_corr = np.ones_like(corr)\n",
    "\n",
    "    for image in range(num_images):\n",
    "        for block_idx, adj_list in zip(range(nodes_per_img), adj_lists):\n",
    "            true_block_idx = block_idx + (nodes_per_img * image)\n",
    "            true_adj_list = list(map(lambda x: x + (nodes_per_img * image), adj_list))\n",
    "            localized_corr[true_block_idx] = np.mean(corr[true_adj_list], axis = 0)\n",
    "    return localized_corr\n",
    "\n",
    "def average_across_diagonal(a):\n",
    "    return (a + a.T) / 2\n",
    "def softmax(x):\n",
    "    y = np.exp(x - np.max(x))\n",
    "    f_x = y / np.sum(np.exp(x))\n",
    "    return f_x\n",
    "def weighted_random_walk(transition_matrix, start_node, iterations = 1000, walks = 100):\n",
    "    transition_matrix = np.array(transition_matrix)\n",
    "    visit_count = np.zeros(transition_matrix.shape[0])\n",
    "    for _ in range(walks):\n",
    "        curr = start_node # np.random.randint(transition_matrix.shape[0]) # Pick Random Starting Node\n",
    "        for _ in range(iterations): \n",
    "            visit_count[curr] += 1\n",
    "            curr = random.choices(\n",
    "                population=range(transition_matrix.shape[0]),\n",
    "                 weights=transition_matrix[curr],\n",
    "                 k=1\n",
    "             )[0]\n",
    "    return visit_count / visit_count.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = lambda x: f\"/Users/walkenz1/Datasets/SALSA/train/cam_{int(x)}/cam_{int(x)}.mp4\"\n",
    "flow_path = lambda x: f\"/Users/walkenz1/Datasets/SALSA/train/cam_{int(x)}/cam_{int(x)}.flow.mp4\"\n",
    "coords_path = lambda x: f\"/Users/walkenz1/Datasets/SALSA/train/cam_{int(x)}/coords_fib_cam_{int(x)}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APL Smart Campus Multi-Camera Tracking Update A\n",
    "#### By: Nile Walker on 12/11/21\n",
    "Weighted Correlation Aggregation for establishing regions of overlap across static multi camera systems\n",
    "\n",
    "## Objective Recap:\n",
    "* Given footage from multiple views throughout a scene, identify individuals traveling through the scene and maintain that identity across all available perspectives in which they appear.\n",
    "\n",
    "## Assumptions:\n",
    "* The footage is from static cameras.\n",
    "* The real world overlap, positionings and calibrations of the cameras are not provided.\n",
    "* The footage is synchronized such that frame n from any camera will represent roughly the same real world time as frame n from any other camera.\n",
    "* Ground Truth multi camera track-lets are extremely limited and may not be available at all.\n",
    "\n",
    "## Current Problem to Solve:\n",
    "### Where did I come from?\n",
    "As an instance-level recognition problem, person re-ID faces two major challenges. First, the intra-class (instance/identity) variations are typically big due to the changes of camera viewing conditions. For instance, the view change across cameras (front to back) brings large appearance changes, making matching the same person difficult. Second, there are also small inter-class variations â€“ people in public space often wear similar clothing; from a distance as typically in surveillance videos,they can look incredibly similar.\n",
    "\n",
    "Given this it would be useful if we could leverage the fact that we're observing a realtime surveillance system inorder to place additional constraints on which identities can be linked. And our assumptions provide several...\n",
    "\n",
    "* Identities observed at the same time in the same camera cannot be the same person.\n",
    "* Under most circumstances the likelihood that two seperate images contain the same person is inversely proportional to the amount of time that has passed since the most recent image was captured.\n",
    "* Security cameras are often placed along passageways and common areas in such a way that the particular sequence of cameras that an individual might pass through is predictable.\n",
    "\n",
    "While the first two are fairly simple to apply the last one requires that we either have some information on the real world placement or relationships of each camera which according to our assumptions we don't. And without enough multi camera tracks to train from we can't just observe which cameras people tend to reappear in. So in-order to get around this we need some unsupervised method to quantify which camera B, identities from camera A are likely to appear in simultaneously or after an allowed gap time. \n",
    "\n",
    "## Current Solutions in Code:\n",
    "So what I choose to do is leverage the fact that the footage is synchronized, and the relationship between motion of objects in overlapping cameras will usually remain consistent if they stay in the same position and are observing the same real world object. So by looking at the correlated motion over time, we can build some idea of how likely an object in camera A is related to an object in camera B. Then in order to better apply these associations to new detections we break the image into patches so that if only certain areas of the image overlap we don't build unwarranted correlations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage every patch of a camera is treated as an individual sensor in which it provides a magnitude and a direction. I grabbed every 12th frame from a camera and then build out an array so that I can compute a correlation coefficients between each of the \"sensors\". At the same time I also compute corelations between the occupancy of each patch which at the moment is represented as the presence of any motion in the block.\n",
    "\n",
    "<!--\n",
    "  import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "demo_ids = [0,2]\n",
    "cameras_flow = [cv2.VideoCapture(flow_path(cam_id)) for cam_id in demo_ids]\n",
    "cameras = [cv2.VideoCapture(video_path(cam_id)) for cam_id in demo_ids]\n",
    "\n",
    "trows, tcols = 3,3\n",
    "\n",
    "figure, axes = plt.subplots(nrows=len(demo_ids), ncols=trows * tcols)\n",
    "figure.set_size_inches(18.5, 10.5, forward=True)\n",
    "for row, (feed, flow_feed) in zip(axes,zip(cameras,cameras_flow)):\n",
    "    ret,img = [feed.read() for i in range(24)][-1]\n",
    "    ret,img_flow = [flow_feed.read() for i in range(24)][-1]\n",
    "    \n",
    "    tiles = tile_img(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), trows, tcols)\n",
    "    flows = tile_img(img_flow, trows, tcols)\n",
    "    \n",
    "    for col, (img, flow) in zip(row, zip(tiles, flows)):\n",
    "        flow[flow==0] = 125\n",
    "        added_image = cv2.addWeighted(img,0.4,flow,0.7,0)\n",
    "        col.axis('off')\n",
    "        col.imshow(added_image)\n",
    "\n",
    "figure.tight_layout(h_pad = 0)\n",
    "\n",
    "_ = [camera.release() for camera in cameras]\n",
    "_ = [camera.release() for camera in cameras_flow]\n",
    "-->\n",
    "<img src = \"images/sample_flow.png\"><img/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_ids = [2, 3]\n",
    "\n",
    "rows, cols = 7, 7\n",
    "\n",
    "cameras = [cv2.VideoCapture(flow_path(cam_id)) for cam_id in cam_ids]\n",
    "\n",
    "X_components = []\n",
    "Y_components = []\n",
    "Z_components = [] \n",
    "\n",
    "interval = 12\n",
    "samples = 250 \n",
    "running = True\n",
    "while running and samples > 0: \n",
    "    samples -= 1\n",
    "    X_components.append([])\n",
    "    Y_components.append([])\n",
    "    Z_components.append([])\n",
    "\n",
    "    for i, feed in enumerate(cameras):\n",
    "        ret, img = [feed.read() for _ in range(interval)][-1]\n",
    "        if not ret or not running:\n",
    "            running = False\n",
    "            break\n",
    "\n",
    "        tiles = tile_img(img, rows, cols)\n",
    "        for tile in tiles:\n",
    "            tile = tile.astype('float32')\n",
    "            mag, ang = cv2.cartToPolar(tile[...,0], tile[...,1])\n",
    "            tile[...,0], tile[...,1] = mag, ang\n",
    "            avg_val_per_row = np.average(tile, axis=0)\n",
    "            avg_val = np.average(avg_val_per_row, axis=0)\n",
    "            X_components[-1].append(avg_val[0])\n",
    "            Y_components[-1].append(avg_val[1])\n",
    "            # Using the presence of any motion as a naive substitute for occupancy \n",
    "            Z_components[-1].append(np.any(tile[tile > 0]))\n",
    "\n",
    "# Generate occupancy measurs\n",
    "X = np.array(X_components[:-1])\n",
    "Y = np.array(Y_components[:-1])\n",
    "Z = np.array(Z_components[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Dataframes are better than numpy arrays at not producing nan values in the correlation matrix \n",
    "corr_X = np.nan_to_num(pd.DataFrame(X).corr().to_numpy())\n",
    "corr_Y = np.nan_to_num(pd.DataFrame(Y).corr().to_numpy())\n",
    "corr_Z = np.nan_to_num(pd.DataFrame(Z).corr().to_numpy())\n",
    "\n",
    "\n",
    "corr = (abs(corr_X) * .25) + (abs(corr_Y)* .25) + (abs(corr_Z)* .5) \n",
    "for _ in range(0):\n",
    "    corr = localize_corr_matrix(corr, len(cam_ids), rows, cols, include_self = False)\n",
    "    \n",
    "# Convert Image Patch Graph\n",
    "corr = average_across_diagonal(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.23023648, 0.06525851, ..., 0.05799291, 0.03781158,\n",
       "        0.08143438],\n",
       "       [0.23023648, 1.        , 0.24400587, ..., 0.1206636 , 0.13356318,\n",
       "        0.08027522],\n",
       "       [0.06525851, 0.24400587, 1.        , ..., 0.15764894, 0.23964499,\n",
       "        0.15039676],\n",
       "       ...,\n",
       "       [0.05799291, 0.1206636 , 0.15764894, ..., 1.        , 0.35407363,\n",
       "        0.08576147],\n",
       "       [0.03781158, 0.13356318, 0.23964499, ..., 0.35407363, 1.        ,\n",
       "        0.2886947 ],\n",
       "       [0.08143438, 0.08027522, 0.15039676, ..., 0.08576147, 0.2886947 ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapped_indexes = [0,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64395604, 0.73919414, 0.7970696 , 0.69084249, 0.85128205,\n",
       "       0.41904762, 0.55311355, 0.72014652, 0.36043956, 0.36630037,\n",
       "       0.34725275, 0.57362637, 0.67912088, 0.62930403, 0.74505495,\n",
       "       0.63589744, 0.64029304, 0.60952381, 0.31062271, 0.59267399,\n",
       "       0.79120879, 0.7992674 , 0.78241758, 0.66080586, 0.66520147,\n",
       "       0.68644689, 0.83296703, 0.75311355, 0.68864469, 0.81465201,\n",
       "       0.69230769, 0.86153846, 0.93113553, 0.8029304 , 0.74358974,\n",
       "       0.62124542, 0.83150183, 0.89304029, 0.9970696 , 0.91282051,\n",
       "       0.8043956 , 0.52234432, 0.62051282, 0.88937729, 0.80879121,\n",
       "       0.83736264, 0.84249084, 0.75531136, 0.53479853, 0.66813187,\n",
       "       0.71062271, 0.71868132, 0.72820513, 0.76483516, 0.70622711,\n",
       "       0.41318681, 0.66666667, 0.6996337 , 0.7970696 , 0.81684982,\n",
       "       0.79413919, 0.77875458, 0.56336996, 0.65128205, 0.94578755,\n",
       "       0.7970696 , 0.77655678, 0.80879121, 0.84395604, 0.73626374,\n",
       "       0.69304029, 0.93186813, 0.87252747, 0.84249084, 0.86520147,\n",
       "       0.91794872, 0.82637363, 0.64102564, 0.803663  , 0.8959707 ,\n",
       "       0.95970696, 0.89010989, 1.        , 0.84175824, 0.69010989,\n",
       "       0.75018315, 0.85641026, 0.99194139, 0.96776557, 0.99340659,\n",
       "       0.70769231, 0.77875458, 0.6974359 , 0.83809524, 0.89450549,\n",
       "       0.92014652, 0.85860806, 0.66520147])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_random_walk(corr,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Ergys Ristani and Carlo Tomasi. Features for multi-target\n",
    "multi-camera tracking and re-identification. In Proceedings of the IEEE conference on computer vision and pattern\n",
    "[]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
