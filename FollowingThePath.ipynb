{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Segmented Trajectories for Abnormal Behavior Detection in Crowded Scenes\n",
    "### Introduction\n",
    "\n",
    "[1]\n",
    "Behavior analysis in crowded scenes remains an open problem in computer vision\n",
    "due to the inherent complexity and vast diversity found in such scenes. One\n",
    "hurdle, that must be overcome, is finding good ways to identify normal travel patterns\n",
    "without tracking individual objects, which is both impractical and unnecessary in\n",
    "crowded areas where occlusion from other individuals is common.\n",
    "\n",
    "[3]\n",
    "G. Santhiya and K. Sankaragomathi proposed two\n",
    "approaches for crowd detection such as direct approach and\n",
    "indirect approach[4]. Direct approach is a detection based\n",
    "method that detects each individual person in a scenes using\n",
    "segmentation or human detection. Indirect approach is map\n",
    "based approaches where it detect visual features are mapped to\n",
    "the number of people. These methods improves understanding\n",
    "of crowd activates and better pedestrian safety. \n",
    "\n",
    "Dongping\n",
    "Zhang and Huailiang Peng used machine learning based\n",
    "method used for crowd scenes detection whether particular\n",
    "scenes are normal or abnormal such as panic, fight and\n",
    "stampede*[5]. Min Sun and Dongping Zhang proposed solution\n",
    "of mixed behavior problem using label distribution algorithm.\n",
    "Mixed behavior means more than two behaviors occurred at a\n",
    "same time. So, at a same time only one behavior is detected\n",
    "and ignores other behaviors. Example of mixed behavior is if\n",
    "fight behavior occurs then tumble or panic behavior also\n",
    "occurred*[3].\n",
    "\n",
    "[Me]\n",
    "Individual and Crowd Level behavior detection methods are limited by a lack of training data that include crowded or semi-crowded scenes.\n",
    "\n",
    "With this in mind I've focused on a statistical approach to anomaly detection \n",
    "\n",
    "Within Statistical Methods\n",
    "[2]\n",
    "Classifies anomoly detection into two categories;\n",
    "trajectory analysis and motion analysis. Trajectory analysis\n",
    "is based on object tracking and typically requires an un\n",
    "crowded environment to operate. [So that detectors can maintain an identity]\n",
    "\n",
    "Motion analysis is better suitable for crowded scenes,[focus on pixel level motion ] and analyses patterns of\n",
    "movement rather than attempting to distinguish specific objects.\n",
    "\n",
    "\n",
    "[1]\n",
    "NOTE IDK if this applies to \n",
    "Another hurdle is finding good ways to understand\n",
    "changes in behavior when the scene context and crowd dynamics can vary over\n",
    "such a wide range.\n",
    "\n",
    "\n",
    "The absence of and diffulty in generating labeled crowd events. Is a considerable obstacle in the progression of such approaches. \n",
    "\n",
    "\n",
    "I use a modified form of the MDF distance metric proposed in [Reference] to compute a distance describing \n",
    "\n",
    "<!-- Some of the few existing works consider the relationship\n",
    "between pedestrians' social behaviours and their walking\n",
    "scenarios. Recently, some methods [1], [2] utilize crowd\n",
    "flow and semantic scene knowledge to detect abnormal\n",
    "activity and obtained good results. But these methods can be\n",
    "only applied for some simple scene (e.g. single sink/source,\n",
    "single crowd flow). There have been a few attempts to\n",
    "model larger groups of people, crowds, which are mostly\n",
    "based on discriminative classifiers *[3]. -->\n",
    "\n",
    "The analysis of crowd movements and behaviour is of\n",
    "particular interest in surveillance domain *[4]. In scenarios\n",
    "where hundreds of cameras are monitored by a few\n",
    "operators behavioural analysis of crowds is useful as a tool\n",
    "for video pre-screening. Activity and event detection has\n",
    "gained much attention in automated video surveillance,\n",
    "content-understanding and content-ranking *[5, 6].\n",
    "[Mention the deep learning thing]\n",
    "\n",
    "Activity detection identifies the actions of a target based\n",
    "on a series of observations and interactions with the\n",
    "environment. To enrich the realistic characteristics, both\n",
    "contextual and social semantic effects should be considered\n",
    "in formulating the crowd behaviour. Inspired by the social\n",
    "behaviour modeling *[7], people are driven by their own\n",
    "\n",
    "\n",
    "Several methods based on optical flow have been presented in recent years\n",
    "to handle these hurdles. In computer vision, optical flow is widely used to compute pixel wise instantaneous motion between consecutive frames, and numerous\n",
    "methods are reported to efficiently compute accurate optical flow. However, optical flow does not capture long-range temporal dependencies, since it is based\n",
    "on just two frames, and by itself does not represent spatial and temporal features\n",
    "of a flow that are useful for general applications to many problems that are approached through optical flow.\n",
    "Streaklines are well known in flow visualization *[3,4] and fluid mechanics *[5] as a tool for measurement and analysis of the flow. With regard to flow visualization, streaklines are defined as the traces of a colored material in the flow.\n",
    "\n",
    "To understand streaklines, consider a fluid flow with an ink dye injected at a\n",
    "particular point. If the ink is continuously injected, then a line will be traced\n",
    "out by the ink in the direction of the flow, this is a streakline. If the direction of\n",
    "flow changes, then the streaklines change accordingly.\n",
    "Streaklines are new to computer vision research. In thi\n",
    "\n",
    "Streaklines provide a means to recognize spatial and temporal changes in the\n",
    "flow, that neither streamlines nor pathlines could provide directly. This point is\n",
    "made here using streak flow and potential functions. In essence, streak flow is\n",
    "obtained by time integration of the velocity field, while potential functions are\n",
    "obtained from spatial integration, and each provides useful information concerning the dynamics in the scene\n",
    "\n",
    "Building on the fluid dynamics approach to crowd motion, we employ another\n",
    "concept from fluids providing a different point of view. In simplified mathematical models of fluids, it is often assumed that the fluid is imcompressible, and irrotational. These assumptions imply several conservation properties of the fluid, but most importantly, they lead to potential functions, which are scalar\n",
    "functions that characterize the flow in a unique way. For this discourse, potential functions enable accurate classification of behaviors in a scene, which is not\n",
    "possible with streak flow alone. Application of potential functions to abnormal\n",
    "behavior detection is presented in Sections 4 and 5\n",
    "\n",
    "\n",
    "In order to compare segmented trajectories as new idenetities are introduced or tracking gaps. I propose a modified version of the MDF () distance utilized in the Quick Bundles Algorithm \n",
    "\n",
    "So in the past few months I've been developing the capability to do ananomolous crowd behaviour detection \n",
    "\n",
    "Rapid Changes in crowd density  \n",
    "\n",
    "Rapid changes in crowd speed \n",
    "\n",
    "Esablish norm on the crowd level\n",
    "\n",
    "### References \n",
    "\n",
    "* [1] https://link.springer.com/content/pdf/10.1007%2F978-3-642-15558-1_32.pdf\n",
    "* [2] https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7019309\n",
    "* [3] https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8275999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|||\n",
    "|------|------|\n",
    "|<img src=\"docs/images/Stampede.png\" width=\"300px\"></img>|<img src=\"docs/images/JayWalk.png\" width=\"300px\"></img>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach uses optical flow averaged over prior frames to propose several possible representative trajectories. And then uses a clustering algorithm to simplify those results.\n",
    "\n",
    "|Normal Travel|Streakline Generation|Path Simplification|\n",
    "|------|------|------|\n",
    "|<img src=\"docs/images/final-flow.png\" width=\"300px\"></img>|<img src=\"docs/images/path-traveled.png\" width=\"300px\"></img>|<img src=\"docs/images/path-traveled.png\" width=\"300px\"></img>|\n",
    "\n",
    "\n",
    "This path simplification allows us to to better extract high level features like path occupancy and average speed along a path.\n",
    "\n",
    "Extracting Representative Trajectories\n",
    "\n",
    "Comparing Segmented Trajectories \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = lambda x: f\"/Users/walkenz1/Datasets/CAP_ONE/test/cam_{int(x)}/cam_{int(x)}.mp4\"\n",
    "flow_path = lambda x: f\"/Users/walkenz1/Datasets/CAP_ONE/test/cam_{int(x)}/cam_{int(x)}.flow.mp4\"\n",
    "coords_path = lambda x: f\"/Users/walkenz1/Datasets/CAP_ONE/test/cam_{int(x)}/coords_fib_cam_{int(x)}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APL Smart Campus Crowd Anomoly Detection\n",
    "#### By: Nile Walker on 12/11/21\n",
    "<!-- Weighted Correlation Aggregation for establishing regions of overlap across static multi camera systems -->\n",
    "\n",
    "## Objective Recap:\n",
    "* Given footage from multiple views throughout a scene, identify individuals traveling through the scene and maintain that identity across all available perspectives in which they appear.\n",
    "\n",
    "## Assumptions:\n",
    "* The footage is from static cameras.\n",
    "* The real world overlap, positionings and calibrations of the cameras are not provided.\n",
    "* The footage is synchronized such that frame n from any camera will represent roughly the same real world time as frame n from any other camera.\n",
    "* Ground Truth multi camera track-lets are extremely limited and may not be available at all.\n",
    "\n",
    "## Current Problem to Solve:\n",
    "### Where did I come from?\n",
    "As an instance-level recognition problem, person re-ID faces two major challenges. First, the intra-class (instance/identity) variations are typically big due to the changes of camera viewing conditions. For instance, the view change across cameras (front to back) brings large appearance changes, making matching the same person difficult. Second, there are also small inter-class variations â€“ people in public space often wear similar clothing; from a distance as typically in surveillance videos,they can look incredibly similar.\n",
    "\n",
    "Given this it would be useful if we could leverage the fact that we're observing a realtime surveillance system inorder to place additional constraints on which identities can be linked. And our assumptions provide several...\n",
    "\n",
    "* Identities observed at the same time in the same camera cannot be the same person.\n",
    "* Under most circumstances the likelihood that two seperate images contain the same person is inversely proportional to the amount of time that has passed since the most recent image was captured.\n",
    "* Security cameras are often placed along passageways and common areas in such a way that the particular sequence of cameras that an individual might pass through is predictable.\n",
    "\n",
    "While the first two are fairly simple to apply the last one requires that we either have some information on the real world placement or relationships of each camera which according to our assumptions we don't. And without enough multi camera tracks to train from we can't just observe which cameras people tend to reappear in. So in-order to get around this we need some unsupervised method to quantify which camera B, identities from camera A are likely to appear in simultaneously or after an allowed gap time. \n",
    "\n",
    "## Current Solutions in Code:\n",
    "So what I choose to do is leverage the fact that the footage is synchronized, and the relationship between motion of objects in overlapping cameras will usually remain consistent if they stay in the same position and are observing the same real world object. So by looking at the correlated motion over time, we can build some idea of how likely an object in camera A is related to an object in camera B. Then in order to better apply these associations to new detections we break the image into patches so that if only certain areas of the image overlap we don't build unwarranted correlations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage every patch of a camera is treated as an individual sensor in which it provides a magnitude and a direction. I grabbed every 12th frame from a camera and then build out an array so that I can compute a correlation coefficients between each of the \"sensors\". At the same time I also compute corelations between the occupancy of each patch which at the moment is represented as the presence of any motion in the block.\n",
    "\n",
    "<!--\n",
    "  import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "demo_ids = [0,2]\n",
    "cameras_flow = [cv2.VideoCapture(flow_path(cam_id)) for cam_id in demo_ids]\n",
    "cameras = [cv2.VideoCapture(video_path(cam_id)) for cam_id in demo_ids]\n",
    "\n",
    "trows, tcols = 3,3\n",
    "\n",
    "figure, axes = plt.subplots(nrows=len(demo_ids), ncols=trows * tcols)\n",
    "figure.set_size_inches(18.5, 10.5, forward=True)\n",
    "for row, (feed, flow_feed) in zip(axes,zip(cameras,cameras_flow)):\n",
    "    ret,img = [feed.read() for i in range(24)][-1]\n",
    "    ret,img_flow = [flow_feed.read() for i in range(24)][-1]\n",
    "    \n",
    "    tiles = tile_img(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), trows, tcols)\n",
    "    flows = tile_img(img_flow, trows, tcols)\n",
    "    \n",
    "    for col, (img, flow) in zip(row, zip(tiles, flows)):\n",
    "        flow[flow==0] = 125\n",
    "        added_image = cv2.addWeighted(img,0.4,flow,0.7,0)\n",
    "        col.axis('off')\n",
    "        col.imshow(added_image)\n",
    "\n",
    "figure.tight_layout(h_pad = 0)\n",
    "\n",
    "_ = [camera.release() for camera in cameras]\n",
    "_ = [camera.release() for camera in cameras_flow]\n",
    "-->\n",
    "<img src = \"images/sample_flow.png\"><img/>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
